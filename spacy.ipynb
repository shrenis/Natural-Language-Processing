{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHRENI SHAH-19MAI0038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPACY INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python. \n",
    "- spaCy is designed specifically for production use and helps you build applications that process and ‚Äúunderstand‚Äù large volumes of text. \n",
    "- It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
    "- spaCy is used to perform Tokenization, Part-of-speech (POS) Tagging, Lemmatization, Sentence Boundary Detection (SBD), Dependency Parsing, Named Entity Recognition (NER), Entity Linking (EL) wtc things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the english library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# create doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High:[ 3.379025    0.60030574 -1.8544984 ]\n",
      "Explosive:[ 3.4047315  1.4757392 -0.5115589]\n",
      "Research:[-1.2007543   1.8710369   0.80947626]\n",
      "was:[-3.1372113 -1.5912277 -2.8670561]\n",
      "the:[-2.4285593 -3.782106   3.7539806]\n",
      "independent:[2.359838  1.7131125 0.5008454]\n",
      "British:[3.426497   0.46152067 0.09366858]\n",
      "project:[-4.5285845  4.971984  -1.6442893]\n",
      "to:[-3.784699    0.23621476 -2.8847935 ]\n",
      "develop:[-2.4502208  -0.15967411 -0.39966428]\n",
      "atomic:[2.0508976  1.114049   0.51341105]\n",
      "bombs:[ 1.1723139  0.7761539 -1.0135481]\n",
      "after:[ 1.6361983   0.18040827 -0.8765584 ]\n",
      "the:[ 0.6448693 -1.2734318  1.8607215]\n",
      "Second:[3.9483616  0.18734695 0.87496614]\n",
      "World:[-1.5217814  3.42406   -1.1666031]\n",
      "War:[ 1.7223792 -1.2408535 -0.860064 ]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"High Explosive Research was the independent British project to develop atomic bombs after the Second World War\")\n",
    "for token in doc:\n",
    "    print(\"{}:{}\".format(token, token.vector[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"any help?please.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - here i have made one documnet on which i will perform some operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any:[ 2.573865  -1.0350318  1.1211486]\n",
      "help?please:[ 0.93766063 -1.5132332   0.07575023]\n",
      ".:[-0.676624  -2.7627392 -1.8642435]\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(\"{}:{}\".format(token, token.vector[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Here we ar getting first three entries of vectors for each of the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('any', 'DET'), ('help?please', 'X'), ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text,token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - this shows part of speech tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Linguistic annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey INTJ intj\n",
      ", PUNCT punct\n",
      "I PRON nsubj\n",
      "am AUX ROOT\n",
      "shreni ADJ attr\n",
      ". PUNCT punct\n",
      "can VERB aux\n",
      "not PART neg\n",
      "believe VERB ROOT\n",
      "? PUNCT punct\n",
      "yeah INTJ intj\n",
      "me PRON ROOT\n",
      "too ADV advmod\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hey,I am shreni. can not believe? yeah me too.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Here I am getting every token and its description. ie for comma(,) i am getting PUNCT which entions punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey\n",
      ",\n",
      "I\n",
      "am\n",
      "shreni\n",
      ".\n",
      "can\n",
      "not\n",
      "believe\n",
      "?\n",
      "yeah\n",
      "me\n",
      "too\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Tokenization is splitting up a sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n",
    " - by doin token.text we can get each tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey hey INTJ UH intj Xxx True False\n",
      ", , PUNCT , punct , False False\n",
      "I -PRON- PRON PRP nsubj X True True\n",
      "am be AUX VBP ROOT xx True True\n",
      "shreni shreni ADJ JJ attr xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "can can VERB MD aux xxx True True\n",
      "not not PART RB neg xxx True True\n",
      "believe believe VERB VB ROOT xxxx True False\n",
      "? ? PUNCT . punct ? False False\n",
      "yeah yeah INTJ UH intj xxxx True False\n",
      "me -PRON- PRON PRP ROOT xx True True\n",
      "too too ADV RB advmod xxx True True\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - here we are extraction every partof speech and tag them.\n",
    " - spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore _ to its name.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lemminflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'examples'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('I am testing this example.')\n",
    "doc[2]._.lemma()         \n",
    "doc[4]._.inflect('NNS')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watched'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('what will you watch with me dude?')\n",
    "#doc[5]._.lemma()         \n",
    "doc[3]._.inflect('VBD')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - lemmatization gives dictionary form of a single token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peach\n",
      "emoji\n",
      "üçë\n",
      "emoji. It's outranking eggplant\n",
      "Peach\n",
      "Peach is the superior emoji.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(\"Peach emoji is where it has always been. Peach is the superior \"\n",
    "          \"emoji. It's outranking eggplant üçë \")\n",
    "print(doc[0].text)          # 'Peach'\n",
    "print(doc[1].text)          # 'emoji'\n",
    "print(doc[-1].text)         # 'üçë'\n",
    "print(doc[13:19].text)      # 'outranking eggplant'\n",
    "\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "print(noun_chunks[0].text)  # 'Peach emoji'\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "assert len(sentences) == 3\n",
    "print(sentences[1].text)    # 'Peach is the superior emoji.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This shows subset of given document. \n",
    "- we can get the partwe want. for eg, here i have mentioned 13:19 which will return words from 13 to 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peach Peach nsubj emoji\n",
      "it it nsubj been\n",
      "Peach Peach nsubj is\n",
      "It It nsubj 's\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 17.056751 False\n",
      "cat True 18.97264 False\n",
      "bananassaa True 20.019207 False\n",
      "afskfsd True 19.65514 False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "tokens = nlp(\"dog cat bananassaa afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.4192831\n",
      "dog banana 0.4178361\n",
      "cat dog 0.4192831\n",
      "cat cat 1.0\n",
      "cat banana 0.34277543\n",
      "banana dog 0.4178361\n",
      "banana cat 0.34277543\n",
      "banana banana 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHRENI\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")  \n",
    "tokens = nlp(\"dog cat banana\")\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this word similarity shows what is the probability of having same word. \n",
    "- It compares each word with another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple <-> banana 0.37282813\n",
      "pasta <-> hippo 0.3430258\n",
      "True True True True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHRENI\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \n",
      "C:\\Users\\SHRENI\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
    "\n",
    "apple = doc[0]\n",
    "banana = doc[2]\n",
    "pasta = doc[6]\n",
    "hippo = doc[8]\n",
    "\n",
    "print(\"apple <-> banana\", apple.similarity(banana))\n",
    "print(\"pasta <-> hippo\", pasta.similarity(hippo))\n",
    "print(apple.has_vector, banana.has_vector, pasta.has_vector, hippo.has_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It shows the simillarity between two given words. \n",
    "- which words' comparison gets higher numbers, are most probabily similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab, hashes and lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n",
      "coffee\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(\"I love coffee\")\n",
    "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
    "print(doc.vocab.strings[3197928453018144401])  # 'coffee'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whenever possible, spaCy tries to store data in a vocabulary, the Vocab, that will be shared by multiple documents.\n",
    "- To save memory, spaCy also encodes all strings to hash values \n",
    "- Here ‚Äúcoffee‚Äù has the hash value 3197928453018144401. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4690420944186131903 X I I True False True en\n",
      "love 3702023516439754181 xxxx l ove True False False en\n",
      "coffee 3197928453018144401 xxxx c fee True False False en\n"
     ]
    }
   ],
   "source": [
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in above given example we found hash value of coffee, same for other word we got the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES \n",
    "\n",
    "- https://www.guru99.com/nltk-tutorial.html\n",
    "- https://spacy.io/usage/models\n",
    "- https://monkeylearn.com/blog/natural-language-processing-tools/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GITHUB LINK\n",
    "- https://github.com/shrenis/Natural-Language-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
